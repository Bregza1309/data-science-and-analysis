{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8e04e23-0923-4716-b817-8f22369e14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c44ec3-17ba-4be8-84f3-7bea9384d981",
   "metadata": {},
   "source": [
    "## STEMMING\n",
    "<p>It is the process of finding the root word, hence reduce conflicts and convert words\n",
    "to their base words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d2ed57-2d13-49b7-921f-b5daf2b947f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish', 'fish', 'fish']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishwords = ['fish','Fishing','Fishes']\n",
    "prt = nltk.PorterStemmer()\n",
    "[prt.stem(ts) for ts in fishwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6dd0e-82a0-4122-af6f-0c92b40e56fd",
   "metadata": {},
   "source": [
    "<h2>Lemmatization</h2>\n",
    "<p>It is the process to convert words into their actual dictionary form.In nltk\n",
    "WordNetLemmatizer() is present to do this task</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c95450c-8cb9-4a35-bdec-daa8019465b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish', 'Fishings', 'Fishes']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fishwords = ['fishes','Fishings','Fishes']\n",
    "WNLemma = nltk.WordNetLemmatizer()\n",
    "[WNLemma.lemmatize(ts) for ts in fishwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a6ef4-4af0-412e-8b96-931e6591a6dd",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>\n",
    "<p>This can be done by split() function available in python. But if we want to do it\n",
    "more clearly, we can use nltk tokenization</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0da6d4-5e65-4ae5-aaa9-17f58f977c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why', 'are', 'you', 'so', 'intelligent?', 'Bregz@', '1309']\n",
      "['Why', 'are', 'you', 'so', 'intelligent', '?', 'Bregz', '@', '1309']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Why are you so intelligent? Bregz@ 1309\"\n",
    "words = text2.split(' ')\n",
    "print(words)\n",
    "print(nltk.word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69717830-8dce-4301-a24a-be09409b48d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His name John', ' He lives in U', 'K', ' with his wife Is he the best? No he is not!']\n",
      "['His name John.', 'He lives in U.K. with his wife Is he the best?', 'No he is not!']\n"
     ]
    }
   ],
   "source": [
    "text3 = \"His name John. He lives in U.K. with his wife Is he the best? No he is not!\"\n",
    "sent1 = text3.split(\".\")\n",
    "print(sent1)\n",
    "sent2 = nltk.sent_tokenize(text3)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2c688-f90f-449f-a669-12525bf17ab1",
   "metadata": {},
   "source": [
    "<p>Another task that covered under Natural Language Processing is Part of Speech\n",
    "(POS) Tagging. This task basically tags the words in the sentence with noun, pronoun,\n",
    "verb, adjective etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0906a731-e0fe-4165-bd29-64053416ed77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Why', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'IN'),\n",
       " ('intelligent', 'JJ'),\n",
       " ('?', '.'),\n",
       " ('Bregz', 'NNP'),\n",
       " ('@', 'NN'),\n",
       " ('1309', 'CD')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using text2\n",
    "tokenized_words = nltk.word_tokenize(text2)\n",
    "nltk.pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90149159-5eca-44a6-9c34-cf267b174bde",
   "metadata": {},
   "source": [
    "## TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05220572-7eb7-4159-9efe-e1efaf994ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeb3a6a0-564b-4cc3-bced-2d88336efa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import newsgroup data\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1c9448a-bb45-4bc3-bcf2-435cac94863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fetch_20newsgroups(subset = 'train',shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab9d6382-e59f-4b60-9161-fa86516ffed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe1cff41-cb2c-4ea8-af11-5bd3ae44db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d1194c7-938a-4518-900c-b5fca6c98791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert above sentences to word vectors using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0472da-a954-4948-9a63-e93fc0eefe82",
   "metadata": {},
   "source": [
    "count_vector = CountVectorizer()\n",
    "count_vector.fit(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b6e4d4f-1e64-40e6-815d-136868745954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "count_vector = CountVectorizer()\n",
    "count_vector.fit(train_data.data)\n",
    "train_count = count_vector.transform(train_data.data)\n",
    "print(train_count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88dbad-5ee3-4fca-865c-17fb30a8acd3",
   "metadata": {},
   "source": [
    "<p>In above matrix we are storing only count of words. We can weight them based on\n",
    "importance. We can use TF-IDF to do that. We need to import TFIDF</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b7a07ff-7b4e-42d5-8933-69227d07c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990ccfa-3d1c-4b26-9fbc-d9ecdd4d586f",
   "metadata": {},
   "source": [
    "<p>Create Object and fit</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eb360c5-c99c-40ad-a4fb-e3df9a3922a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model = TfidfTransformer()\n",
    "tfidf_model.fit(train_count)\n",
    "train_tfidf = tfidf_model.transform(train_count)\n",
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e032c5-e486-49a6-8dc2-678df71211bd",
   "metadata": {},
   "source": [
    "<p>We need to create model object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1702c585-8c20-405b-b418-325bc31e0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBModel = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f751d-1cc7-4c13-bda4-614a56380e1e",
   "metadata": {},
   "source": [
    "<p>Fit model on training data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9374846-b32c-4908-9c12-18a620ebfd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBModel.fit(train_tfidf,train_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd3a0b-5609-4772-aa2c-c66b145f1bf4",
   "metadata": {},
   "source": [
    "<p>Now to check performance of our model we can use test data available in the\n",
    "same dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b31a7e20-8e13-4d59-b537-6fb5f0032ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data =  fetch_20newsgroups(subset='test', shuffle=True)\n",
    "type(test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b58be2-dc09-458c-8bc7-e04074400d44",
   "metadata": {},
   "source": [
    "<p>Convert Testing data in the correct format</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d2c05df-606b-4a0c-9238-21a0199e90f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 130107)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_count = count_vector.transform(test_data.data)\n",
    "test_tfidf = tfidf_model.transform(test_count)\n",
    "test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e8a2b-7783-4498-9da1-198c1752eacc",
   "metadata": {},
   "source": [
    "<p>Now, we transform testing data in the correct form. We can predict the target using\n",
    "predict() function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7b7a229-811a-4bf1-aec0-e2b62770b4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 11,  0, ...,  9,  3, 15])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = NBModel.predict(test_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fda958-2d80-42f5-978f-9f010fb95358",
   "metadata": {},
   "source": [
    "<p>We can evaluate our model using accuracy score. First, we import it by using scikit\n",
    "learn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "025c70ee-befe-4715-877b-60d699ffd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0473e36-31f1-4a35-a7c7-3278a2abb24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :77.39%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy :{:.2f}%'.format(accuracy_score(test_data.target,predicted)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8b8f490-e77f-4af8-af92-ffbeef2a613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0962e724-bff9-4ba7-bc5d-fe6ba951ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaner(raw_data):\n",
    "    cleaned_data = []\n",
    "    for row in raw_data:\n",
    "        stemmed_words =[]\n",
    "        tokenized = nltk.word_tokenize(row)\n",
    "        prt = nltk.PorterStemmer()\n",
    "        for token in tokenized:\n",
    "            if token not in stop_words:\n",
    "                if(token.isalnum() == True):\n",
    "                    stemmed_words.append(prt.stem(token))\n",
    "        sent = ' '.join(stemmed_words)\n",
    "        cleaned_data.append(sent)\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74cb10b1-9bf6-4ac5-9c17-a736587ac37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mod = dataCleaner(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "874636b4-265a-4bee-b25e-83ead4f918ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acc8f42e-3aaa-483b-99b0-bae8d42136b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_mod = dataCleaner(test_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf726dfb-5dee-4175-801d-7671fe934668",
   "metadata": {},
   "source": [
    "<p>Transform data to prepare for model training</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "277fb154-8945-4276-8476-5c1ce99b0554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 77399)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(train_data_mod)\n",
    "trainmod_count = count_vector.transform(train_data_mod)\n",
    "testmod_count = count_vector.transform(test_data_mod)\n",
    "tfidf_model.fit(trainmod_count)\n",
    "trainmod_tfidf = tfidf_model.transform(trainmod_count)\n",
    "testmod_tfidf = tfidf_model.transform(testmod_count)\n",
    "testmod_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5d67b-8c49-4f68-aa99-964935d1726f",
   "metadata": {},
   "source": [
    "<p>Build and fit model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7540e1a7-22e7-4c07-bf58-d7d09043bb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBModel_mod  = MultinomialNB()\n",
    "NBModel_mod.fit(trainmod_tfidf,train_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cf537-ac09-49ee-98ed-5eb5cd539b74",
   "metadata": {},
   "source": [
    "<p>Make Predictions and Evaluate Model Perfomance</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "713fed8c-6dcc-4df9-a58c-933815a4b482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:80.28\n"
     ]
    }
   ],
   "source": [
    "pred = NBModel_mod.predict(testmod_tfidf)\n",
    "print('Accuracy:{:.2f}'.format(accuracy_score(test_data.target,pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff64564-cf1a-4aaa-a1f5-21be00566ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
